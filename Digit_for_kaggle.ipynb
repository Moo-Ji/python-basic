{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.15663830935955048\n",
      "정확도 : 0.9730973732243473\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tf.reset_default_graph()  #tensorflow 그래프 리셋\n",
    "\n",
    "#1. data loading\n",
    "data=pd.read_csv(\"./data/digit/train.csv\",sep=\",\")\n",
    "data_2=pd.read_csv(\"./data/digit/test.csv\",sep=\",\")\n",
    "data_2=np.array(data_2.values, dtype=np.float32)\n",
    "#data_2=np.float32(data_2)\n",
    "\n",
    "sess=tf.Session()\n",
    "num_data=int(data.shape[0]*0.7)\n",
    "data_train=data.loc[0:num_data,:]\n",
    "data_test=data.loc[num_data:,:]\n",
    "\n",
    "train_x=np.array(data_train.iloc[:,1:785].values)\n",
    "train_y=sess.run(tf.one_hot(data_train.iloc[:,0],10)) \n",
    "\n",
    "test_x=np.array(data_test.iloc[:,1:785].values)\n",
    "test_y=sess.run(tf.one_hot(data_test.iloc[:,0],10)) \n",
    "\n",
    "# #2. placeholder\n",
    "X=tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "keep_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "X_img = tf.reshape(X, shape=[-1,28,28,1]) #데이터의 개수 : -1은계산을 맡김\n",
    "#filter 생성\n",
    "W1=tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "\n",
    "#convolution\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
    "#relu\n",
    "L1=tf.nn.relu(L1)\n",
    "#max pooling\n",
    "L1= tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "#3.2 convolution layer2\n",
    "L2=tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu)\n",
    "L2=tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "#convolution layer 끝\n",
    "\n",
    "\n",
    "#4.FC(Neural Network)\n",
    "L2=tf.reshape(L2, shape=[-1, 7*7*64]) \n",
    "\n",
    "#5.weight & bias\n",
    "W2=tf.get_variable(\"weight2\", shape=[7*7*64,256]\n",
    "                   , initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "_layer1=tf.nn.relu(tf.matmul(L2,W2)+b2)\n",
    "layer1=tf.nn.dropout(_layer1, keep_prob=keep_rate)\n",
    "\n",
    "W3=tf.get_variable(\"weight3\", shape=[256,256]\n",
    "                   , initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([256]),name=\"bias3\")\n",
    "_layer2=tf.nn.relu(tf.matmul(layer1,W3)+b3)\n",
    "layer2=tf.nn.dropout(_layer2, keep_prob=keep_rate)   #keep_prob : (여기서는 keep_rate를 0.5로 지정해줌) 50%의 뉴론이 drop out되도록\n",
    "                                                     #           만약 0.7이면 30%의 뉴론이 drop out\n",
    "\n",
    "W4=tf.get_variable(\"weight4\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([10]),name=\"bias4\")\n",
    "\n",
    "#Hypothesis\n",
    "H=tf.matmul(layer2,W4)+b4\n",
    "\n",
    "#Cost Fuction\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=H,labels=Y))\n",
    "\n",
    "#train\n",
    "train=tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "#session, 초기화\n",
    "#sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#학습\n",
    "train_epoch=1\n",
    "batch_size=100\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(train_x.shape[0] / batch_size)  #반복횟수 : 전체데이터 / batch_size\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = train_x[i*batch_size:i*batch_size+batch_size]\n",
    "        ,train_y[i*batch_size:i*batch_size+batch_size]  \n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y,keep_rate:0.5}) \n",
    "        #batch_x, batch_y = data_x[i*batch_size:i*batch_size+batch_size],data_y[i*batch_size:i*batch_size+batch_size]      #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        \n",
    "        \n",
    "    print(\"cost: {}\".format(cost_val))\n",
    "\n",
    "#정확도 측정\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_sum(tf.cast(correct, dtype=tf.float32))\n",
    "result_sum=0\n",
    "num_of_iter = int(test_x.shape[0]/batch_size)\n",
    "    \n",
    "for i in range(num_of_iter):\n",
    "    batch_x, batch_y = test_x[i*batch_size:i*batch_size+batch_size],test_y[i*batch_size:i*batch_size+batch_size] \n",
    "    correct_num=sess.run(accuracy, feed_dict={X:batch_x,Y:batch_y,keep_rate:1})\n",
    "    result_sum+=correct_num\n",
    "    \n",
    "print(\"정확도 : {}\".format(result_sum/test_x.shape[0]))\n",
    "# final=sess.run(predict, feed_dict={X:data_2, keep_rate:1 })\n",
    "# print(\"예측값\",final)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=100\n",
    "num_of_iter = int(data_2.shape[0]/batch_size)\n",
    "final=[]\n",
    "for i in range(num_of_iter):\n",
    "    batch_x= data_2[i*batch_size:i*batch_size+batch_size] \n",
    "    #print(\"예측값\",sess.run(predict, feed_dict={X:batch_x, keep_rate:1}))\n",
    "    values=sess.run(predict, feed_dict={X:batch_x, keep_rate:1})\n",
    "    final.append(values.tolist())\n",
    "final=np.array(final)\n",
    "final=np.ravel(final)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=100\n",
    "num_of_iter = int(data_2.shape[0]/batch_size)\n",
    "final=[]\n",
    "for i in range(num_of_iter):\n",
    "    batch_x= data_2[i*batch_size:i*batch_size+batch_size] \n",
    "    #print(\"예측값\",sess.run(predict, feed_dict={X:batch_x, keep_rate:1}))\n",
    "    values=sess.run(predict, feed_dict={X:batch_x, keep_rate:1})\n",
    "    final.append(values.tolist())\n",
    "    \n",
    "\n",
    "final=np.array(final)\n",
    "final=np.ravel(final)\n",
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_3=pd.read_csv(\"./data/digit/sample_submission.csv\",sep=\",\")\n",
    "\n",
    "data_3[\"Label\"]=final\n",
    "\n",
    "data_3.to_csv(\"Digit_B\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train data를 이용해서 해당 모델을 학습\n",
    "- 최종적으로 얻어내는 것은 Hypothesis\n",
    "- H를 이용해서 예측값 도출\n",
    "- test data(입력 feature)가 들어감\n",
    "- H=>[0.342, 0,234 ...]    가설 값 : 이 그림에 대한 픽셀값(10개로 구성, 다 더하면 1)을 모델을 통해 돌려보니 나오는 예측값(=확률값)\n",
    "- 이렇게 나온 값(H)에 argmax를 취하면 가장 큰 값의 index를 얻어낼 수 있고 이 값을 입력 label의 argmax와 비교해서 \n",
    "- 만약 같으면 예측이 잘 되었다. 틀리면 예측 모델이 이상함\n",
    "\n",
    "- ensemble(앙상블)\n",
    "- 모델이 여러개 ex10개\n",
    "- 각각의 모델의 학습시킴\n",
    "- 각 모델의 입력 파라미터(이미지 픽셀)을 넣어서 예측값을 알아냄\n",
    "- Model1=>[0.342, 0,234 ...]  Model1=>[0.234, 0,64 ...]   Model1=>[0.52, 0,29 ...] ...model10\n",
    "- 각 열을 sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- self=현재 자기자신을 가르키는 인스턴스\n",
    "- self.name = 이 클래스의 객체가 가지는 변수\n",
    "- 클래스 안에 정의되어 있는 함수의 인자 첫번째는 self를 줘야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class Student:\n",
    "    def __init__(self, name, kor, eng, math):\n",
    "        self.name=name\n",
    "        self.kor=kor\n",
    "        self.eng=eng\n",
    "        self.math=math\n",
    "        \n",
    "    def calc_avg(self):\n",
    "        return(self.kor+self.eng+self.math)/3\n",
    "    \n",
    "stu1=Student(\"홍길동\", 10,20,30)\n",
    "print(stu1.calc_avg())\n",
    "stud2=Student(\"김길동\", 30,40,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#여러개의 모델을 만들어서 관리를 해야 함\n",
    "#각각의 모델을 class의 instance로 만들어 관리\n",
    "#우리가 만들 model의 데이터와 기능을 생각해서 class를 디자인\n",
    "\n",
    "class CNNModel:\n",
    "    #사용하는 데이터를 field(변수로 선언)\n",
    "    #사용하는 기능은 함수(method)로 표현\n",
    "    #-데이터를 로딩하는 기능은 넣지 않는다 => 공용으로 사용하기 때문(메모리 낭비가 심하다)\n",
    "    #-tensorflow graph를 그리는 기능(모델을 구축)\n",
    "    #-학습하는 기능\n",
    "    #-정확도 측정\n",
    "    #-예측작업\n",
    "    def __init__(self, name, sess):\n",
    "        self.name=name\n",
    "        self.sess=sess\n",
    "    def build_model(self):\n",
    "        #placeholder부터 코드 작성(~train)\n",
    "        #self.train=tf.train.Adamop~\n",
    "    def exec_train(self):\n",
    "        #sess.run([self.train,cost], feed_dict={})\n",
    "        \n",
    "\n",
    "#1. 데이터 로딩\n",
    "#-판다스를 이용해서 데이터를 읽음\n",
    "#2. 모델객체를 생성(10개)\n",
    "#-sess=tf.Session()\n",
    "mode=Model(\"model1\",sess)\n",
    "modle1.build_model()\n",
    "model1.exec_train(x_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.10792142152786255\n",
      "정확도 : 0.9720657090707087\n",
      "Tensor(\"add_2:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tf.reset_default_graph()  #tensorflow 그래프 리셋\n",
    "\n",
    "#1. data loading\n",
    "data=pd.read_csv(\"./data/digit/train.csv\",sep=\",\")\n",
    "data_2=pd.read_csv(\"./data/digit/test.csv\",sep=\",\")\n",
    "data_2=np.array(data_2.values, dtype=np.float32)\n",
    "#data_2=np.float32(data_2)\n",
    "\n",
    "sess=tf.Session()\n",
    "num_data=int(data.shape[0]*0.7)\n",
    "data_train=data.loc[0:num_data,:]\n",
    "data_test=data.loc[num_data:,:]\n",
    "\n",
    "train_x=np.array(data_train.iloc[:,1:785].values)\n",
    "train_y=sess.run(tf.one_hot(data_train.iloc[:,0],10)) \n",
    "\n",
    "test_x=np.array(data_test.iloc[:,1:785].values)\n",
    "test_y=sess.run(tf.one_hot(data_test.iloc[:,0],10)) \n",
    "\n",
    "# #2. placeholder\n",
    "X=tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "keep_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "X_img = tf.reshape(X, shape=[-1,28,28,1]) #데이터의 개수 : -1은계산을 맡김\n",
    "#filter 생성\n",
    "W1=tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "\n",
    "#convolution\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
    "#relu\n",
    "L1=tf.nn.relu(L1)\n",
    "#max pooling\n",
    "L1= tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "#3.2 convolution layer2\n",
    "L2=tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu)\n",
    "L2=tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "#convolution layer 끝\n",
    "\n",
    "\n",
    "#4.FC(Neural Network)\n",
    "L2=tf.reshape(L2, shape=[-1, 7*7*64]) \n",
    "\n",
    "#5.weight & bias\n",
    "W2=tf.get_variable(\"weight2\", shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "_layer1=tf.nn.relu(tf.matmul(L2,W2)+b2)\n",
    "layer1=tf.nn.dropout(_layer1, keep_prob=keep_rate)\n",
    "\n",
    "W3=tf.get_variable(\"weight3\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([256]),name=\"bias3\")\n",
    "_layer2=tf.nn.relu(tf.matmul(layer1,W3)+b3)\n",
    "layer2=tf.nn.dropout(_layer2, keep_prob=keep_rate)   #keep_prob : (여기서는 keep_rate를 0.5로 지정해줌) 50%의 뉴론이 drop out되도록\n",
    "                                                     #           만약 0.7이면 30%의 뉴론이 drop out\n",
    "\n",
    "W4=tf.get_variable(\"weight4\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([10]),name=\"bias4\")\n",
    "\n",
    "#Hypothesis\n",
    "H=tf.matmul(layer2,W4)+b4\n",
    "\n",
    "#Cost Fuction\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=H,labels=Y))\n",
    "\n",
    "#train\n",
    "train=tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "#session, 초기화\n",
    "#sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#학습\n",
    "train_epoch=1\n",
    "batch_size=100\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(train_x.shape[0] / batch_size)  #반복횟수 : 전체데이터 / batch_size\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = train_x[i*batch_size:i*batch_size+batch_size],train_y[i*batch_size:i*batch_size+batch_size]  \n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y,keep_rate:0.5}) \n",
    "        #batch_x, batch_y = data_x[i*batch_size:i*batch_size+batch_size],data_y[i*batch_size:i*batch_size+batch_size]      #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        \n",
    "        \n",
    "    print(\"cost: {}\".format(cost_val))\n",
    "\n",
    "#정확도 측정\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_sum(tf.cast(correct, dtype=tf.float32))\n",
    "result_sum=0\n",
    "num_of_iter = int(test_x.shape[0]/batch_size)\n",
    "    \n",
    "for i in range(num_of_iter):\n",
    "    batch_x, batch_y = test_x[i*batch_size:i*batch_size+batch_size],test_y[i*batch_size:i*batch_size+batch_size] \n",
    "    correct_num=sess.run(accuracy, feed_dict={X:batch_x,Y:batch_y,keep_rate:1})\n",
    "    result_sum+=correct_num\n",
    "    \n",
    "print(\"정확도 : {}\".format(result_sum/test_x.shape[0]))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x=np.array(data_test.iloc[:,1:785].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
