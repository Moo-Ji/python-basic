{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "#and 연산 \n",
    "\n",
    "x_anddata= [[0,0],\n",
    "            [0,1],\n",
    "            [1,0],\n",
    "            [1,1]]\n",
    "y_anddata=[[0],[0],[0],[1]]\n",
    "\n",
    "print(y_anddata)\n",
    "\n",
    "X = tf.placeholder(shape=[None,2],dtype = tf.float32  ) #행 숫자는 무상관\n",
    "Y = tf.placeholder(shape=[None,1],dtype = tf.float32  )\n",
    "\n",
    "W= tf.Variable(tf.random_normal([2,1]), name=\"weight\"  )\n",
    "b= tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "\n",
    "logit = tf.matmul(X,W)+b\n",
    "\n",
    "H=tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( logits=logit , labels=Y ))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_anddata,Y:y_anddata} )\n",
    "    if step%300 == 0 :\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "\n",
    "predict = tf.cast(H>0.5 ,dtype=tf.float32) #원래는 정수를 실수로 , 실수를 정수로 바꿔주는 cast 타입변환 1.0 0.0 으로 변환인듯?\n",
    "                                            #0.5보다 크면 1로 떨어짐\n",
    "correct = tf.equal(predict,Y) #실데이터와 예측한 것을 비교하는것 \n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 ={}\".format(sess.run(accuracy ,feed_dict={X:x_anddata,Y:y_anddata} ) ))\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:2.7065720558166504\n",
      "cost:0.00013740842405240983\n",
      "cost:8.53808960528113e-05\n",
      "cost:6.267914432100952e-05\n",
      "cost:4.983652615919709e-05\n",
      "cost:4.154649650445208e-05\n",
      "cost:3.5734479752136394e-05\n",
      "cost:3.1429590308107436e-05\n",
      "cost:2.8106787794968113e-05\n",
      "cost:2.5461282348260283e-05\n",
      "정확도 =1.0\n",
      "예측값=[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# ## Multiple layer 를 이용한 XOR문제 해결\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # 1. Training data set\n",
    "\n",
    "# x_data = [[0,0],\n",
    "#          [0,1],\n",
    "#          [1,0],\n",
    "#          [1,1]]\n",
    "\n",
    "# y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "# #2. placeHolder\n",
    "\n",
    "# X = tf.placeholder(shape = [None,2], dtype = tf.float32)\n",
    "# Y = tf.placeholder(shape = [None,1], dtype = tf.float32)\n",
    "\n",
    "# #3. Weight &bias\n",
    "# W1 = tf.Variable(tf.random_normal([2,256]), name = \"whight1\") #한 레이어에서 wide 하게 20개로 늘림. 2->20 로지스틱 숫자 늘리기\n",
    "# b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\") #출력 256개\n",
    "\n",
    "# layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([256,512]), name = \"whight2\") #한 레이어에서 wide 하게 20개로 늘림. 512 = 로지스틱 숫자 , 클수록 정확\n",
    "# b2 = tf.Variable(tf.random_normal([512]), name = \"bias2\") #출력 256개\n",
    "\n",
    "# layer2 = tf.sigmoid(tf.matmul(layer1,W2) + b2) #depts 3개 딥 네트워크. 너무 깊게 하면 오히려 학습이 안되는 경우가 있다. \n",
    "\n",
    "# #학습이 너무 잘되면 over fitting 이 발생, 실제 예측이 엇나감. 적정 선을 유지해야 합니다. \n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([512,1]), name = \"whight3\") #행렬 곱 연산이 일어나도록 수정#입력 256개\n",
    "# b3 = tf.Variable(tf.random_normal([1]), name = \"bias3\") #최종 값은 하나\n",
    "\n",
    "# #4. Hypothesis\n",
    "# logit = tf.matmul(layer2,W3) + b3 \n",
    "# H = tf.sigmoid(logit)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,labels=Y ))\n",
    "\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# sess=tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# for step in range(30000):\n",
    "#     _,cost_val = sess.run([train,cost], feed_dict={X:x_data , Y:y_data})\n",
    "    \n",
    "#     if step%3000==0 :\n",
    "#         print(\"cost:{}\".format(cost_val))\n",
    "\n",
    "# predict = tf.cast(H>0.5 ,dtype=tf.float32) #원래는 정수를 실수로 , 실수를 정수로 바꿔주는 cast 타입변환 1.0 0.0 으로 변환인듯?\n",
    "#                                             #0.5보다 크면 1로 떨어짐\n",
    "# correct = tf.equal(predict,Y) #실데이터와 예측한 것을 비교하는것 \n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "\n",
    "# print(\"정확도 ={}\".format(sess.run(accuracy ,feed_dict={X:x_data,Y:y_data} ) ))\n",
    "\n",
    "\n",
    "# #prediction 예측\n",
    "\n",
    "# print(\"예측값={}\".format(sess.run(predict,feed_dict={X:x_data,Y:y_data})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "3.6376514\n",
      "2.0265338\n",
      "1.2301319\n",
      "1.0275302\n",
      "1.1596653\n",
      "0.6793493\n",
      "0.73589736\n",
      "0.6498156\n",
      "0.49743897\n",
      "0.6619822\n",
      "0.8542\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "logit = tf.matmul(layer2,W3) +b3\n",
    "H = tf.nn.softmax(logit)\n",
    "#지금은 layer추가하고 있다는 점 유의.\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "24.631128\n",
      "4.0394263\n",
      "2.6461957\n",
      "0.64890146\n",
      "0.31953064\n",
      "0.08266126\n",
      "8.842596e-05\n",
      "2.9294455e-05\n",
      "0.0\n",
      "0.0\n",
      "==========\n",
      "0.9449\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0.6762993\n",
      "0.41321594\n",
      "0.3357047\n",
      "0.34630498\n",
      "0.25484332\n",
      "0.18033093\n",
      "0.10422319\n",
      "0.22170722\n",
      "0.093595535\n",
      "0.09794306\n",
      "==========\n",
      "0.9594\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu#Xavier initialization 도입 초기 W값 지정\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "#W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "W1 = tf.get_variable(\"wight1\",shape = [784,256], initializer=  tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "#W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "W2 = tf.get_variable(\"wight2\",shape = [256,512], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "#W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "W3 = tf.get_variable(\"wight3\",shape = [512,10], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0.8976872\n",
      "0.61608267\n",
      "0.25697765\n",
      "0.42058846\n",
      "0.4338282\n",
      "0.18917851\n",
      "0.23240094\n",
      "0.14603838\n",
      "0.15598248\n",
      "0.3351928\n",
      "==========\n",
      "0.9578\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu#Xavier initialization 도입 초기 W값 지정#Drop out(overfitting 방지)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "#W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "keep = tf.placeholder(dtype = tf.float32) #입력 값으로 사용하겠다. 상수로 박지 않고. \n",
    "W1 = tf.get_variable(\"wight1\",shape = [784,256], initializer=  tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep) #keepprob 유지할 확률 0.5~0.7\n",
    "#W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "W2 = tf.get_variable(\"wight2\",shape = [256,512], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep)\n",
    "#W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "W3 = tf.get_variable(\"wight3\",shape = [512,10], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y,keep:0.7})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels, keep:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  12., 1200.,    0.],\n",
       "         [  16., 1600.,    0.]],\n",
       "\n",
       "        [[  24., 2400.,    0.],\n",
       "         [  28., 2800.,    0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np #4차원 배열 직접 만들겁니다. \n",
    "\n",
    "# Image Shape -> (1,3,3,1) -> (이미지 개수, 가로 , 세로, 색) 1은 흑백 #1행 3행, 3행, 1행 각각 표현 \n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                   [[4],[5],[6]],\n",
    "                   [[7],[8],[9]]]], dtype = np.float32)\n",
    "image.shape\n",
    "#filter의 shape => (2,2,1,1)=> (가로, 세로, color, 필터 개서)\n",
    "W = np.array([[[[1,100,-10]],[[1,100,-10]]],[[[1,100,-10]],[[1,100,-10]]]],dtype = np.float32)\n",
    "# layers 이용하면 random으로 잡을 수 있다. \n",
    "\n",
    "conv2d = tf.nn.conv2d(image, W, strides=[1,1,1,1], padding = \"VALID\")\n",
    "conv2d = tf.nn.relu(conv2d)\n",
    "#tf내에서 layers 패키지가 사용하기 더 쉬움. \n",
    "# conv2d = tf.layers.conv2d(inputs=image, filters=32,\n",
    "#                           kernel_size=[2,2],padding =\"valid\",\n",
    "#                          strides = 1,activation = tf.nn.relu) #필터 몇 개 쓸 것인지는 지정해야함. #relu() 안쓰고 함수 지정만. \n",
    "sess = tf.Session()\n",
    "sess.run(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[4.],\n",
       "         [3.]],\n",
       "\n",
       "        [[2.],\n",
       "         [1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#image -> (1,2,2,1)\n",
    "images = np.array([[[[4],[3]],[[2],[1]]]],dtype = np.float32)\n",
    "images.shape\n",
    "\n",
    "pool = tf.nn.max_pool(images, ksize = [1,2,2,1], strides = [1,1,1,1],\n",
    "                     padding = \"SAME\")\n",
    "sess.run(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/Mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/Mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/Mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/Mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###MNIST 이미지 1장으로 Convalution과 MAX Pooling 처리\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/Mnist\",one_hot = True)\n",
    "\n",
    "#2. 처음 한 장의 이미지만 가지고 해보자. \n",
    "img = mnist.train.images[0]\n",
    "# plt.imshow(img.reshape(28,28),cmap = \"Greys\")\n",
    "# plt.show()\n",
    "\n",
    "#3. Convolution \n",
    "#원본데이터의 형태 변경 1차배열 -> 4차배열\n",
    "img = tf.reshape(img, shape = [-1,28,28,1]) #-1 이미지 수로 채워\n",
    "\n",
    "#4. 필터 정의\n",
    "##filter 크기 3*3, 필터 개수 5\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,3,1,5])) #[]커널 가로 세로, 컬러, 필터 수 \n",
    "\n",
    "conv2d = tf.nn.conv2d(img, W, strides = [1,2,2,1], padding = \"SAME\")\n",
    "\n",
    "#padding same 원본과 같은 크기인 것은 스트라이드가 1일 때. 지금 2칸씩일 때는 사이즈 절반으로 줄어든다. \n",
    "\n",
    "\n",
    "#tf.nn.relu\n",
    "conv2d = tf.nn.relu(conv2d) \n",
    "\n",
    "#Max pooling(sub sampling)\n",
    "pool = tf.nn.max_pool(conv2d, ksize = [1,2,2,1], #샘플 크기2by2, 패딩하여 사차원 배열 만들기 위해 앞뒤에 1,1\n",
    "                     strides = [1,2,2,1],#샘플 이동 간격\n",
    "                     padding=\"SAME\")#데이터 유실 방지. \n",
    "#cf. 평균값으로 풀링하는 방법도 있지만, max가 가장 효율이 좋음. \n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "result = sess.run(pool)\n",
    "result.shape #(1, 14, 14, 5) #결과물 원래 1장 이미지, 크기 1414 필터 수에 따라 5장 이미지 생성. \n",
    "##결과 이미지를 보기 편하기 위한 데이터 처리 - 축 변경 (5,14,14,1)\n",
    "#=> (1, 7, 7, 5) strides 2*2해서 작아짐. #해상도는 떨어지지만 특징을 더 잡아냈음. \n",
    "result = np.swapaxes(result,0,3)\n",
    "result.shape #->(5, 14, 14, 1)\n",
    "# for loop로 이미지 뽑을 수 있다. 다섯번 돌리겠다 .\n",
    "\n",
    "fig,axes = plt.subplots(1,5)\n",
    "\n",
    "for idx, t_img in enumerate(result):\n",
    "    axes[idx].imshow(t_img.reshape(7,7), cmap=\"Greys\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/Mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/Mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/Mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/Mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Cost :0.10711287707090378\n",
      "Cost :0.11915435642004013\n",
      "Cost :0.08680844306945801\n",
      "Cost :0.048987120389938354\n",
      "Cost :0.04479017108678818\n",
      "Cost :0.03238360211253166\n",
      "Cost :0.026623820886015892\n",
      "Cost :0.00797225721180439\n",
      "Cost :0.03428422287106514\n",
      "Cost :0.0451173409819603\n",
      "정확도 :0.9905999898910522\n"
     ]
    }
   ],
   "source": [
    "#MNIST with CNN\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/Mnist\", one_hot = True) \n",
    "#원래 폴더 없다면 만들어서 데이터 넣어준다. #one_hot y측 레이블 여러개\n",
    "\n",
    "#2. Placeholder\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    " #데이터 개수 미정, 컬럼 784개 (잘 알려진 예제이다. )\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32)\n",
    "keep_rate = tf.placeholder(dtype = tf.float32) #스칼라이므로 모양 잡을 필요 없다. dropout비율\n",
    "\n",
    "#3. Convolution Layer\n",
    "##3.1 . Layer1 (여러개의 계층으로 구성할 수 있다. )\n",
    "#이미지 데이터를 사차 배열로 바꿔야 함. (이미지 수, 가로, 세로, 색)\n",
    "X_img = tf.reshape(X,shape = [-1,28,28,1]) #[,,,] 4차 배열\n",
    "#텐서로 모양 바꾸면 텐서 타입으로 빠진다. #-1 나머지를 이미지 사이즈로 만들어. 데이터 개수를 계산하지 않을거야\n",
    "\n",
    "#    filter생성\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev = 0.01)) #3*3크기, 깊이 1 흑백, 서른 두개의 필터#표준편차 설정 비슷한 값 도출\n",
    "#    Convolution\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding = \"SAME\") #1*1이고 패딩 작업\n",
    "#    ReLU\n",
    "L1 = tf.nn.relu(L1)\n",
    "#    Max Pooling\n",
    "L1 = tf.nn.max_pool(L1, ksize= [1,2,2,1],strides = [1,2,2,1], padding =\"SAME\" )\n",
    "\n",
    "##3.1 . Layer1 \n",
    "#    Filter,Convolution,ReLU\n",
    "L2 = tf.layers.conv2d(inputs= L1, filters = 64, kernel_size = [3,3],  #패키지가 다름. #ksize정수로 떨어지도록 계산해야한다. \n",
    "                     padding =\"SAME\", strides = 1, activation = tf.nn.relu)\n",
    "#Max pooling\n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size = [2,2], padding=\"SAME\", strides = 2) #ksize와 같은 내용 다른 표현 psize\n",
    "#이미지 크기 반으로 줄어든다. \n",
    "\n",
    "##시간을 절약하려면 dropout 하면 된다. \n",
    "\n",
    "###end Convolution Layer\n",
    "\n",
    "#4. FC(Neural Network)\n",
    "#데이터에 대한 2차 배열 이미지로 바꾸어야 한다. \n",
    "L2 = tf.reshape(L2, shape = [-1, 7*7*64]) #L2.shape(?,7,7,64)\n",
    "\n",
    "#5. Weight & bias\n",
    "W2 = tf.get_variable(\"weight2\", shape= [7*7*64,256],initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W2) + b2)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_rate)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape= [256,256],initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]), name=\"bias3\")\n",
    "\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W3) + b3)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_rate)\n",
    "\n",
    "W4 = tf.get_variable(\"weight4\", shape= [256,10],initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]), name=\"bias4\")\n",
    "\n",
    "#6. Hypothesis\n",
    "H = tf.matmul(layer2,W4) + b4\n",
    "\n",
    "#7. Cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H, labels= Y))\n",
    "\n",
    "#8. Train\n",
    "train = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "\n",
    "#9. Session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#10. 학습 \n",
    "#gpu 쓸 때는 리소스 때문에 배치를 해야한다. \n",
    "\n",
    "num_of_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X:batch_x, Y:batch_y, keep_rate:0.5}) \n",
    "        #0.25 노드 많을 때, 0.7도 사용\n",
    "    \n",
    "    print(\"Cost :{}\".format(cost_val))\n",
    "    \n",
    "#accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict,tf.argmax(Y,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"정확도 :{}\".format(sess.run(accuracy, feed_dict = {X: mnist.test.images, Y: mnist.test.labels, keep_rate:1})))\n",
    "\n",
    "# accuracy = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "# result_sum = 0:\n",
    "#     num_of_iter = int(mnist.test.num_examples/batch_size)\n",
    "    \n",
    "#     for i in range(num_of_iter):\n",
    "#         batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "#         correct_num = sess.run([train, cost], feed_dict={X:batch_x, Y:batch_y, keep_rate:1}) \n",
    "#         result_sum += correct_num\n",
    "# print(\"정확도 :{}\".format(result_sum/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.next_batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/Mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/Mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/Mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/Mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/Mnist\", one_hot = True) \n",
    "\n",
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_sum(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "result_sum = 0:\n",
    "    num_of_iter = int(mnist.test.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        correct_num = sess.run([train, cost], feed_dict={X:batch_x, Y:batch_y, keep_rate:1}) \n",
    "        result_sum += correct_num\n",
    "print(\"정확도 :{}\".format(result_sum/10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Mnist 해결 메커니즘\n",
    " 하나의 모델을 생성\n",
    " CNN(Convolution later, Neural network)\n",
    " Train data를 이용해서 해당 모델을 학습\n",
    " 가설을 만들기 위해 학습을 진행. H를 위해 W와 b값을 찾음. \n",
    " Hypothesis를 이용해서 예측값을 도출할 수 있다. \n",
    " test data(입력 feature)가 해당 모델에 들어간다. \n",
    " H => [0.12, 0.02, 0.33 ... 0.21] \n",
    " SoftMax 를 통해 얻는 최초의 값들 = 예측값 = 확률값. \n",
    " onehotencoding개수 만큼 들어있고, 각각의 확률값. \n",
    " 모두 더하면 1이 나온다. \n",
    " 이 중 가장 큰 값을 찾아 몇번째 있는지 센다. \n",
    " H에 argmax취하면 가장 큰 값의 index를 얻어낼 수 있고, \n",
    " 이 값을 입력 label의 argmax와 비교하여 결과를 얻는다. \n",
    " 만약 같으면 예측이 잘 된 것. \n",
    " 랜덤 값을 이용하기 때문에 학습 결과에 차이가 있음. \n",
    " 여러 개의 모델을 동시에 이용 - 앙상블 2~5% 정도 상승함. \n",
    "    \n",
    "   #ensemble(앙상블)\n",
    "   모델이 여러개. 10개\n",
    "   각각의 모델을 학습시킨다. \n",
    "   각 모델의 입력 파라메터 (이미지 픽셀)을 넣어 예측값을 알아낸다. \n",
    "   모델마다의 H값을 도출. \n",
    "   각 열마다 더한 값에서 다시 argmax취함. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"./Data/Digit/train.csv\", sep = \",\")\n",
    "test = pd.read_csv(\"./Data/Digit/test.csv\", sep=\",\")\n",
    "test = np.array(test.values, dtype = np.float32)\n",
    "\n",
    "sess = tf.Session()\n",
    "seg = int(data.shape[0]*0.7)\n",
    "train_data = data.loc[:seg,:]\n",
    "test_data = data.loc[seg:,:]\n",
    "train_y = tf.one_hot(train_data.iloc[:,0],10)\n",
    "train_y.get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
